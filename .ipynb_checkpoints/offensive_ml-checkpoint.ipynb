{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602abdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics, manifold\n",
    "from tqdm import tqdm\n",
    "import gensim.downloader as gensim_api\n",
    "import transformers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf36c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet\n",
      "0                             je racist if other race\n",
      "1   whenwwourdebit card decliiig athe abortion ann...\n",
      "2                                       ken nein g y \n",
      "3                             si the wort you re loo \n",
      "4    ainerican teenagegirl visiting the children o...\n",
      "5                                               taos \n",
      "6   ijustioundoutmexiean babies come out justilike...\n",
      "7   when the schoolishooter accidently shoots thea...\n",
      "8                                lsiiiniongutimnss s \n",
      "9              on oa oo wa that faggot am lae rat at \n",
      "10                                        ugly becta \n",
      "11                                 nigger up fc el ae\n",
      "12                                       d it faggot \n",
      "13  oh you re a nigger ie ye a please telumehow mu...\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('./ocr/output.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2107691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = gensim_api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86c9512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('slur', 0.5023007392883301),\n",
       " ('niggers', 0.5003144145011902),\n",
       " ('faggot', 0.4708784222602844)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.most_similar([\"nigger\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773a60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(lst_words, top, nlp):\n",
    "    lst_out = lst_words\n",
    "    for tupla in nlp.most_similar(lst_words, topn=top):\n",
    "        lst_out.append(tupla[0])\n",
    "    return list(set(lst_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08569834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarWords(words, top, nlp):\n",
    "    out = words\n",
    "    for tupl in nlp.most_similar(words, topn=top):\n",
    "        out.append(tupl[0])\n",
    "    return list(set(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da4bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters={}\n",
    "clusters['Hate']=get_similar_words(['nigger'], top=30, nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84dae921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at al were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at al.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('al', do_lower_case=True)\n",
    "nlp = transformers.TFDistilBertModel.from_pretrained('al')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cabcd65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_bert_embedding(txt, tokenizer, nlp):\n",
    "    '''\n",
    "    Word embedding with Bert (equivalent to nlp[\"word\"]).\n",
    "    :parameter\n",
    "        :param txt: string \n",
    "        :param tokenizer: transformers tokenizer\n",
    "        :param nlp: transformers bert\n",
    "    :return\n",
    "        tensor sentences x words x vector (1x3x768) \n",
    "    '''\n",
    "    # tokenize sentence to tokens (integers)\n",
    "    idx = tokenizer.encode(txt)\n",
    "    # convert to array of shape (1, num_words+2) - EOS and CLS added\n",
    "    idx = np.array(idx)[None,:]\n",
    "    # generate embeddings for each token - output is a tuple\n",
    "    embedding = nlp(idx)\n",
    "    # select first member of the tuple, remove first dimension which is 1 to get (num_words,embedding size 712)\n",
    "    # exclude CLS and EOS tokens\n",
    "    X = np.array(embedding[0][0][1:-1])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d893fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:02<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## create list of news vector\n",
    "lst_mean_vecs = [utils_bert_embedding(txt, tokenizer, nlp).mean(0) for txt in tqdm(df['tweet'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4162cc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(lst_mean_vecs)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945ccdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"mean_vec1.csv\", X, delimiter=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cccf300c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0788977   0.01371969  0.08699187 ... -0.347426    0.10609712\n",
      "  -0.01698205]\n",
      " [-0.28224272 -0.0515355   0.24325894 ... -0.15808289 -0.09150222\n",
      "   0.3179501 ]\n",
      " [-0.09774822  0.01405543  0.30436674 ... -0.22396998 -0.03229867\n",
      "   0.32713223]\n",
      " ...\n",
      " [-0.00609337 -0.09731167  0.0899381  ... -0.16879795  0.20959683\n",
      "  -0.13552223]\n",
      " [-0.27410337 -0.00712461  0.38595176 ...  0.05722349  0.23717184\n",
      "   0.46486974]\n",
      " [ 0.02506365  0.3005097   0.5438104  ... -0.18173702  0.21164489\n",
      "   0.1650597 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79dc74f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0788977   0.01371969  0.08699187 ... -0.347426    0.10609712\n",
      "  -0.01698205]\n",
      " [-0.28224272 -0.0515355   0.24325894 ... -0.15808289 -0.09150222\n",
      "   0.3179501 ]\n",
      " [-0.09774822  0.01405543  0.30436674 ... -0.22396998 -0.03229867\n",
      "   0.32713223]\n",
      " ...\n",
      " [-0.00609337 -0.09731167  0.0899381  ... -0.16879795  0.20959683\n",
      "  -0.13552223]\n",
      " [-0.27410337 -0.00712461  0.38595176 ...  0.05722349  0.23717184\n",
      "   0.46486974]\n",
      " [ 0.02506365  0.30050969  0.54381043 ... -0.18173702  0.21164489\n",
      "   0.1650597 ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('mean_vec1.csv', delimiter='/')\n",
    "print(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab13dfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dic_y = {k:utils_bert_embedding(v, tokenizer, nlp).mean(0) for k,v in tqdm(clusters.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c10a883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Hate'])\n"
     ]
    }
   ],
   "source": [
    "print(dic_y.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f110330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_NAN_inf_values(x):\n",
    "    '''Replace NaN with zero and infinity with large finite numbers'''\n",
    "    if len(np.where(np.isnan(X))[0])>0 or len(np.where(np.isnan(X))[1])>0:\n",
    "        return np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b31beb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.array(\n",
    "            [metrics.pairwise.cosine_similarity(X, [dic_y['Hate']]).T.tolist()[0]]\n",
    "            ).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2506e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49741834]\n",
      " [0.55123788]\n",
      " [0.52380407]\n",
      " [0.45625421]\n",
      " [0.4873828 ]\n",
      " [0.33289388]\n",
      " [0.52327996]\n",
      " [0.51811409]\n",
      " [0.4871144 ]\n",
      " [0.48822236]\n",
      " [0.46024731]\n",
      " [0.47092256]\n",
      " [0.46745193]\n",
      " [0.56664884]]\n"
     ]
    }
   ],
   "source": [
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a11f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf=pd.read_csv('full_data/hate_speech_mlma/hate_speech_mlma/fr_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "075197d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    abusive\n",
      "1                  offensive\n",
      "2                    hateful\n",
      "3                     normal\n",
      "4                     normal\n",
      "                ...         \n",
      "4009               offensive\n",
      "4010    disrespectful_normal\n",
      "4011               offensive\n",
      "4012                  normal\n",
      "4013               offensive\n",
      "Name: sentiment, Length: 4014, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dtf['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "318538f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abusive_offensive\n"
     ]
    }
   ],
   "source": [
    "z=np.array(dtf['sentiment'])\n",
    "print(z[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2b5e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c801d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant=(z == 0).sum()+(z == 1).sum()\n",
    "# print(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5f55171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "retrieved=(similarities>0.65).sum()\n",
    "print(retrieved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0783cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp=0\n",
    "# for i in range (0,4000):\n",
    "#     if(similarities[i]>0.50 and z[i]<2):\n",
    "#         tp=tp+1\n",
    "# print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15daf0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision=tp/retrieved\n",
    "# print(\"Precision is\",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcb11322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn=0\n",
    "# for i in range (0,4000):\n",
    "#     if(similarities[i]<0.50 and z[i]<2):\n",
    "#         fn=fn+1\n",
    "# recall=tp/(fn+tp)\n",
    "# print(\"Recall is\",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2c5e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp=retrieved-tp\n",
    "# tn=0\n",
    "# for i in range (0,4000):\n",
    "#     if(similarities[i]<0.50 and z[i]==2):\n",
    "#         tn=tn+1\n",
    "# accuracy=(tp+tn)/(tp+tn+fn+fp)\n",
    "# print(\"Accuracy is\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ee410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dca86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
